# app\rag\rag_pipeline.py

"""
M√≥dulo principal para el pipeline RAG completo con Chroma
"""
from typing import List, Dict, Any, Optional, Union, Tuple
import logging
import os
import asyncio

from langchain.docstore.document import Document
from langchain.schema.embeddings import Embeddings
from langchain.schema.retriever import BaseRetriever
from langchain_community.vectorstores import Chroma
from langchain_core.language_models import BaseLanguageModel

from behemot_framework.rag.document_loader import DocumentLoader
from behemot_framework.rag.processors import DocumentProcessor
from behemot_framework.rag.embeddings import EmbeddingManager
from behemot_framework.rag.vector_store import VectorStoreManager
from behemot_framework.rag.retriever import RAGRetriever


logger = logging.getLogger(__name__)


class RAGPipeline:
    """Clase principal para gestionar el pipeline RAG completo"""
    
    def __init__(
        self,
        embedding_provider: str = "openai",
        embedding_model: str = "text-embedding-3-small",
        persist_directory: str = "chroma_db",
        collection_name: str = "default_collection",
        client_settings: Optional[Any] = None,
        storage_type: str = "chroma",
        redis_url: Optional[str] = None,
    ):
        """
        Inicializa el pipeline RAG
        
        Args:
            embedding_provider: Proveedor de embeddings ('openai', 'huggingface', 'google')
            embedding_model: Modelo de embeddings a usar
            persist_directory: Directorio para persistencia de Chroma
            collection_name: Nombre de la colecci√≥n
            client_settings: Configuraci√≥n opcional del cliente Chroma
            storage_type: Tipo de almacenamiento ('chroma' o 'redis')
            redis_url: URL de conexi√≥n a Redis (requerido si storage_type='redis')
        """
        self.embedding_provider = embedding_provider
        self.embedding_model = embedding_model
        self.persist_directory = persist_directory
        self.storage_type = storage_type
        self.redis_url = redis_url
        self.collection_name = collection_name
        self.client_settings = client_settings
        
        # Preparar par√°metros seg√∫n el proveedor
        embedding_params = {"provider": embedding_provider}
        
        if embedding_provider == "openai":
            embedding_params["model"] = embedding_model
        elif embedding_provider == "huggingface":
            embedding_params["model_name"] = embedding_model
        elif embedding_provider in ["google", "gemini"]:
            embedding_params["model"] = embedding_model
            
        self.embeddings = EmbeddingManager.get_embeddings(**embedding_params)
        
        self.vectorstore = None
        
        # Inicializar vectorstore seg√∫n el tipo
        if self.storage_type == "redis":
            if self.redis_url:
                try:
                    self.vectorstore = VectorStoreManager.load_redis_index(
                        self.embeddings, 
                        self.redis_url,
                        collection_name
                    )
                    logger.info(f"Colecci√≥n Redis '{collection_name}' cargada correctamente")
                except Exception as e:
                    logger.error(f"Error al cargar la colecci√≥n Redis: {e}")
                    # Continuar sin vectorstore, se crear√° cuando sea necesario
            else:
                logger.warning("Redis URL no configurada, no se puede cargar el √≠ndice")
        else:  # chroma (default)
            if os.path.exists(persist_directory):
                try:
                    self.vectorstore = VectorStoreManager.load_chroma_index(
                        self.embeddings, 
                        persist_directory, 
                        collection_name,
                        client_settings=client_settings
                    )
                    logger.info(f"Colecci√≥n '{collection_name}' cargada correctamente desde {persist_directory}")
                except Exception as e:
                    logger.error(f"Error al cargar la colecci√≥n: {e}")
    
    def ingest_documents(
        self,
        sources: Union[str, List[str]],
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        splitter_type: str = "recursive",
    ) -> Chroma:
        """
        Ingiere documentos en el pipeline RAG
        
        Args:
            sources: Ruta(s) al documento o documentos
            chunk_size: Tama√±o de los chunks
            chunk_overlap: Superposici√≥n entre chunks
            splitter_type: Tipo de divisor ('token', 'recursive', 'character')
            
        Returns:
            Vectorstore Chroma con los documentos ingeridos
        """
        logger.info(f"Iniciando ingesti√≥n de documentos desde {sources}")
        
        # Asegurar que sources sea una lista
        if isinstance(sources, str):
            sources = [sources]
        
        # Cargar documentos
        all_documents = []
        load_errors = []
        for source in sources:
            try:
                docs = DocumentLoader.load_document(source)
                if docs:
                    all_documents.extend(docs)
                    logger.info(f"‚úÖ Cargados {len(docs)} documentos desde {source}")
                else:
                    load_errors.append(f"No se encontraron documentos en {source}")
                    logger.warning(f"‚ö†Ô∏è No se encontraron documentos en {source}")
            except Exception as e:
                load_errors.append(f"Error al cargar {source}: {e}")
                logger.error(f"‚ùå Error al cargar {source}: {e}")
        
        if not all_documents:
            error_msg = f"No se pudieron cargar documentos. Errores: {'; '.join(load_errors)}"
            logger.error(f"‚ùå {error_msg}")
            # En lugar de crear un vectorstore vac√≠o, lanzar excepci√≥n
            raise ValueError(error_msg)
        
        # Procesar y dividir documentos
        try:
            chunks = DocumentProcessor.process_documents(
                all_documents,
                splitter_type=splitter_type,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
            )
            
            if not chunks:
                error_msg = f"El procesamiento de documentos no gener√≥ chunks. Verificar contenido de archivos y configuraci√≥n de splitter."
                logger.error(f"‚ùå {error_msg}")
                raise ValueError(error_msg)
                
            logger.info(f"‚úÖ Documentos procesados en {len(chunks)} chunks")
            
        except Exception as e:
            error_msg = f"Error en el procesamiento de documentos: {e}"
            logger.error(f"‚ùå {error_msg}")
            raise ValueError(error_msg)
        
        # Crear o actualizar vectorstore seg√∫n el tipo
        try:
            if self.storage_type == "redis":
                if self.vectorstore is None:
                    self.vectorstore = VectorStoreManager.create_redis_index(
                        chunks, 
                        self.embeddings, 
                        self.redis_url,
                        self.collection_name
                    )
                    logger.info(f"‚úÖ √çndice Redis creado con {len(chunks)} chunks")
                else:
                    self.vectorstore = VectorStoreManager.add_documents_to_redis(
                        self.vectorstore, chunks
                    )
                    logger.info(f"‚úÖ Agregados {len(chunks)} chunks al √≠ndice Redis existente")
            else:  # chroma (default)
                if self.vectorstore is None:
                    self.vectorstore = VectorStoreManager.create_chroma_index(
                        chunks, 
                        self.embeddings, 
                        self.persist_directory,
                        self.collection_name,
                        self.client_settings
                    )
                    logger.info(f"‚úÖ √çndice Chroma creado con {len(chunks)} chunks")
                else:
                    self.vectorstore = VectorStoreManager.add_documents(
                        self.vectorstore, chunks
                    )
                    logger.info(f"‚úÖ Agregados {len(chunks)} chunks al √≠ndice Chroma existente")
            
            # Validar que el vectorstore se cre√≥ correctamente
            if self.vectorstore is None:
                error_msg = "El vectorstore no se pudo crear correctamente"
                logger.error(f"‚ùå {error_msg}")
                raise ValueError(error_msg)
                
            logger.info(f"üéØ Ingesti√≥n completada exitosamente")
            return self.vectorstore
            
        except Exception as e:
            error_msg = f"Error al crear/actualizar vectorstore: {e}"
            logger.error(f"‚ùå {error_msg}")
            raise ValueError(error_msg)
    
    async def aingest_documents(
        self,
        sources: Union[str, List[str]],
        chunk_size: int = 1000,
        chunk_overlap: int = 200,
        splitter_type: str = "recursive",
    ) -> Chroma:
        """
        Versi√≥n as√≠ncrona de ingest_documents
        """
        # Esta es una implementaci√≥n sencilla que ejecuta la versi√≥n s√≠ncrona
        # En una implementaci√≥n m√°s avanzada, cada parte ser√≠a realmente as√≠ncrona
        return await asyncio.to_thread(
            self.ingest_documents,
            sources,
            chunk_size,
            chunk_overlap,
            splitter_type,
        )
    
    def get_retriever(
        self,
        search_type: str = "similarity",
        search_kwargs: Optional[Dict[str, Any]] = None,
    ) -> BaseRetriever:
        """
        Obtiene un retriever para el vectorstore actual
        
        Args:
            search_type: Tipo de b√∫squeda ('similarity', 'mmr')
            search_kwargs: Par√°metros adicionales para la b√∫squeda
            
        Returns:
            Retriever configurado
        """
        if self.vectorstore is None:
            raise ValueError("No hay vectorstore inicializado, ingiere documentos primero")
            
        return RAGRetriever.get_vectorstore_retriever(
            self.vectorstore,
            search_type=search_type,
            search_kwargs=search_kwargs,
        )
    
    def get_compression_retriever(
        self,
        llm: BaseLanguageModel,
        search_type: str = "similarity",
        search_kwargs: Optional[Dict[str, Any]] = None,
    ) -> BaseRetriever:
        """
        Obtiene un retriever con compresi√≥n para el vectorstore actual
        
        Args:
            llm: Modelo de lenguaje para la compresi√≥n
            search_type: Tipo de b√∫squeda ('similarity', 'mmr')
            search_kwargs: Par√°metros adicionales para la b√∫squeda
            
        Returns:
            Retriever con compresi√≥n configurado
        """
        base_retriever = self.get_retriever(search_type, search_kwargs)
        
        return RAGRetriever.get_compression_retriever(base_retriever, llm)
    
    def query_documents(
        self,
        query: str,
        k: int = 4,
        retriever: Optional[BaseRetriever] = None,
    ) -> List[Document]:
        """
        Consulta documentos usando el vectorstore o un retriever espec√≠fico
        
        Args:
            query: Consulta para la b√∫squeda
            k: N√∫mero de resultados a devolver
            retriever: Retriever opcional (si None, usa b√∫squeda directa)
            
        Returns:
            Lista de documentos relevantes
        """
        if retriever:
            return RAGRetriever.retrieve_documents(retriever, query)
        
        if self.vectorstore is None:
            raise ValueError("No hay vectorstore inicializado, ingiere documentos primero")
            
        return VectorStoreManager.similarity_search(self.vectorstore, query, k=k)
    
    async def aquery_documents(
        self,
        query: str,
        k: int = 4,
        retriever: Optional[BaseRetriever] = None,
    ) -> List[Document]:
        """
        Consulta documentos de forma as√≠ncrona
        
        Args:
            query: Consulta para la b√∫squeda
            k: N√∫mero de resultados a devolver
            retriever: Retriever opcional (si None, usa b√∫squeda directa)
            
        Returns:
            Lista de documentos relevantes
        """
        if retriever:
            return await RAGRetriever.aretrieve_documents(retriever, query)
        
        # Para b√∫squeda directa, ejecutamos la versi√≥n s√≠ncrona en un thread
        return await asyncio.to_thread(self.query_documents, query, k)
    
    def get_formatted_context(self, query: str, k: int = 4) -> str:
        """
        Obtiene contexto formateado para una consulta
        
        Args:
            query: Consulta para la b√∫squeda
            k: N√∫mero de resultados a devolver
            
        Returns:
            Texto formateado con los documentos relevantes
        """
        docs = self.query_documents(query, k)
        return RAGRetriever.format_retrieved_documents(docs)
    
    def delete_collection() -> None:
        """
        Elimina la colecci√≥n actual
        """
        VectorStoreManager.delete_collection(self.persist_directory, self.collection_name)
        self.vectorstore = None
